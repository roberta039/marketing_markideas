import streamlit as st
import google.generativeai as genai
from tavily import TavilyClient
import tempfile
import os

# --- 1. Configurare PaginÄƒ ---
st.set_page_config(
    page_title="Marketing Portfolio Optimizer (Vision)",
    page_icon="ğŸ‘ï¸",
    layout="wide"
)

# --- 2. Gestionare Secrete ---
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
    TAVILY_API_KEY = st.secrets["TAVILY_API_KEY"]
except FileNotFoundError:
    st.error("âš ï¸ Cheile API lipsesc! ConfigureazÄƒ secrets.")
    st.stop()

genai.configure(api_key=GOOGLE_API_KEY)
tavily_client = TavilyClient(api_key=TAVILY_API_KEY)

# --- 3. FuncÈ›ii Helper ---

@st.cache_data(ttl=3600)
def get_available_gemini_models():
    """ReturneazÄƒ modelele care suportÄƒ imagini/fiÈ™iere."""
    models_list = []
    try:
        for m in genai.list_models():
            # CÄƒutÄƒm modele 'gemini' care suportÄƒ generare de conÈ›inut
            if 'generateContent' in m.supported_generation_methods and 'gemini' in m.name:
                models_list.append(m.name)
        models_list.sort(reverse=True)
        return models_list
    except:
        return ["models/gemini-1.5-flash"]

def upload_to_gemini(uploaded_file, mime_type="application/pdf"):
    """ÃncarcÄƒ fiÈ™ierul pe Google AI pentru analizÄƒ vizualÄƒ."""
    try:
        # 1. SalvÄƒm fiÈ™ierul temporar pe disk (Streamlit Ã®l È›ine Ã®n RAM)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        # 2. Ãl Ã®ncÄƒrcÄƒm pe serverele Google
        file_ref = genai.upload_file(tmp_file_path, mime_type=mime_type)
        
        # 3. È˜tergem fiÈ™ierul local temporar
        os.remove(tmp_file_path)
        
        return file_ref
    except Exception as e:
        st.error(f"Eroare la upload cÄƒtre Google: {e}")
        return None

def search_internet(query):
    """CÄƒutare Tavily."""
    try:
        response = tavily_client.search(query=query, search_depth="advanced", max_results=5, include_answer=True)
        context = ""
        if 'answer' in response:
            context += f"Tavily Summary: {response['answer']}\n"
        for res in response.get('results', []):
            context += f"- {res['content']} ({res['url']})\n"
        return context
    except Exception as e:
        return f"Eroare search: {e}"

# --- 4. InterfaÈ›Äƒ ---

st.title("ğŸ‘ï¸ Asistent Marketing (Cu Viziune)")
st.markdown("Acest AI **vede** imaginile din catalog (culori, design, layout) È™i le comparÄƒ cu trendurile de pe net.")

with st.sidebar:
    st.header("âš™ï¸ Configurare")
    
    # Selector Model
    models = get_available_gemini_models()
    selected_model = st.selectbox("Alege Model:", models, index=0, format_func=lambda x: x.replace("models/", "").upper())
    
    st.divider()
    st.header("ğŸ“‚ Catalog")
    uploaded_file = st.file_uploader("ÃncarcÄƒ Catalog PDF", type=['pdf'])
    
    if st.button("Reset Chat"):
        st.session_state.messages = []
        # OpÈ›ional: PoÈ›i È™terge È™i referinÈ›a la fiÈ™ier dacÄƒ vrei
        st.rerun()

# --- 5. LogicÄƒ PrincipalÄƒ ---

if "messages" not in st.session_state:
    st.session_state.messages = []

# Procesare FiÈ™ier (Upload o singurÄƒ datÄƒ la Google)
if uploaded_file:
    # VerificÄƒm dacÄƒ fiÈ™ierul curent este diferit de cel procesat anterior
    if "current_file_name" not in st.session_state or st.session_state.current_file_name != uploaded_file.name:
        with st.spinner("ğŸ“¤ Trimit catalogul cÄƒtre 'ochii' AI-ului (Google Vision)..."):
            google_file_ref = upload_to_gemini(uploaded_file)
            
            if google_file_ref:
                st.session_state.gemini_file = google_file_ref
                st.session_state.current_file_name = uploaded_file.name
                st.success("âœ… Catalog Ã®ncÄƒrcat! AI-ul vede acum imaginile È™i textul.")
            else:
                st.error("Nu s-a putut procesa fiÈ™ierul.")

# Chat UI
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Input
if prompt := st.chat_input("Ex: AratÄƒ designul pixurilor de la pag 5 demodat?"):
    
    if "gemini_file" not in st.session_state:
        st.error("ÃncarcÄƒ catalogul PDF mai Ã®ntÃ¢i.")
    else:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            loader = st.empty()
            with st.spinner("Analizez vizual È™i caut pe net..."):
                
                # 1. CÄƒutare net
                web_data = search_internet(prompt)
                
                # 2. Configurare Prompt Multimodal
                # Ãi trimitem LISTA: [Prompt Text, Obiectul FiÈ™ier PDF]
                input_content = [
                    f"""EÈ™ti un expert Ã®n design de produs È™i marketing.
                    AnalizeazÄƒ fiÈ™ierul PDF ataÈ™at (text È˜I imagini).
                    
                    CONTEXT DIN INTERNET:
                    {web_data}
                    
                    ÃNTREBARE UTILIZATOR:
                    {prompt}
                    
                    INSTRUCÈšIUNI:
                    - Te rog sÄƒ te uiÈ›i la imaginile produselor.
                    - ComenteazÄƒ despre esteticÄƒ, culori È™i design Ã®n raport cu trendurile actuale.
                    - DacÄƒ Ã®ntrebarea e despre o paginÄƒ anume, uitÄƒ-te la acea paginÄƒ.
                    """,
                    st.session_state.gemini_file
                ]
                
                try:
                    model = genai.GenerativeModel(selected_model)
                    response = model.generate_content(input_content, stream=True)
                    
                    full_text = ""
                    for chunk in response:
                        if chunk.text:
                            full_text += chunk.text
                            loader.markdown(full_text + "â–Œ")
                    loader.markdown(full_text)
                    st.session_state.messages.append({"role": "assistant", "content": full_text})
                    
                except Exception as e:
                    loader.error(f"Eroare: {e}")
